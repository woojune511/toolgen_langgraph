import os
import json
from src.agent.graph import build_graph
from src.logger import get_logger
from dsbench_loader import DSBenchLoader
import langchain
from langfuse.langchain import CallbackHandler
from src.config import RESULT_DIR
from src.utils.jupyter_sandbox import AgentSandbox

logger = get_logger("MainExecutor")

langchain.debug = True

def main():
    # 1. ì„¤ì •
    # ì‹¤í–‰ ëª¨ë“œ ì„ íƒ: "analysis" ë˜ëŠ” "modeling"
    MODE = "analysis" 
    DSBENCH_ROOT = "/c1/geonju/project/data/dataset/DSBench"
    
    logger.info(f"Starting DSBench Execution | Mode: {MODE}")

    langfuse_handler = CallbackHandler()
    
    # 2. ë¡œë” ì´ˆê¸°í™”
    try:
        loader = DSBenchLoader(DSBENCH_ROOT, mode=MODE)
    except Exception as e:
        logger.error(f"Failed to initialize loader: {e}")
        return
    
    # 4. ì „ì²´ ë°ì´í„°ì…‹ ìˆœíšŒ (Question ë‹¨ìœ„ ì‹¤í–‰)
    total_tasks = len(loader)
    logger.info(f"Total tasks to process: {total_tasks}")


    # ì¤‘ê°„ê²°ê³¼ê°€ ì¡´ì¬í•˜ë©´ ë¡œë“œ
    if os.path.exists(os.path.join(RESULT_DIR, "result.json")):
        with open(os.path.join(RESULT_DIR, "result.json"), 'r') as f:
            final_answer_dict = json.load(f)
    else:
        final_answer_dict = {}
        
    for i in range(total_tasks):
        # 4-1. ë¬¸ì œ ê°€ì ¸ì˜¤ê¸° (Flattened Question)
        task_data = loader.get_problem(i)
        
        t_id = task_data['id']

        # ê²°ê³¼ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ continue
        if t_id in final_answer_dict and task_data["question_id"] in final_answer_dict[t_id]:
            continue
        
        if not t_id in final_answer_dict:
            final_answer_dict[t_id] = {}

        prompt = task_data['prompt']
        target_dir = task_data['target_dir']
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸš€ Processing Task [{i+1}/{total_tasks}] ID: {t_id}")
        logger.info(f"ğŸ“‚ Work Dir: {target_dir}")
        logger.info(f"{'='*60}")

        with AgentSandbox(work_dir=target_dir) as sandbox:        

            app = build_graph(sandbox)

            # 4-2. ì´ˆê¸° ìƒíƒœ ì„¤ì • (State Injection)
            inputs = {
                "problem": prompt,          # Stuffed Prompt (Intro + Excel Path + Question)
                "work_dir": target_dir,     # (ì¤‘ìš”) ì—ì´ì „íŠ¸ê°€ ì‘ì—…í•  ì ˆëŒ€ ê²½ë¡œ
                "plan": [],
                "current_step_index": 0,
                "decision": "",
                "context_log": [],
                "variable_inventory": {},
                "tool_retrieved": [],
                "tool_generated": [],
                "feedback_history": [],
                "error": None
            }
            
            # 4-3. ê·¸ë˜í”„ ì‹¤í–‰
            try:
                # recursion_limit: ë³µì¡í•œ ë¬¸ì œì¼ìˆ˜ë¡ ë†’ê²Œ ì¡ì•„ì•¼ í•¨ (50~100)
                result = app.invoke(inputs, config={"recursion_limit": 100, "callbacks": [langfuse_handler]})

                final_answer = result.get("final_answer")
                if final_answer:
                    logger.info(f"âœ… Task {t_id} Completed.")
                    final_answer_dict[t_id][task_data["question_id"]] = final_answer
                    
                
                # (ì„ íƒ) ê²°ê³¼ í™•ì¸ ë¡œì§
                # Analysis: result['context_log'] ë§ˆì§€ë§‰ ë‚´ìš© í™•ì¸
                # Modeling: target_dirì— submission.csv ìƒê²¼ëŠ”ì§€ í™•ì¸
                
            except Exception as e:
                logger.error(f"âŒ Task {t_id} Failed: {e}")
                # ì—ëŸ¬ê°€ ë‚˜ë„ ë‹¤ìŒ ë¬¸ì œë¡œ ê³„ì† ì§„í–‰ (Continue)

        with open(os.path.join(RESULT_DIR, "result.json"), 'w') as f:
            json.dump(final_answer_dict, f, indent=4)



def test_single():
    # 1. ì„¤ì •
    # ì‹¤í–‰ ëª¨ë“œ ì„ íƒ: "analysis" ë˜ëŠ” "modeling"
    MODE = "analysis" 
    DSBENCH_ROOT = "/c1/geonju/project/data/dataset/DSBench"

    langchain.debug = True
    langfuse_handler = CallbackHandler()
    
    logger.info(f"Starting DSBench Execution | Mode: {MODE}")
    
    # 2. ë¡œë” ì´ˆê¸°í™”
    try:
        loader = DSBenchLoader(DSBENCH_ROOT, mode=MODE)
    except Exception as e:
        logger.error(f"Failed to initialize loader: {e}")
        return

    # 3. LangGraph ì•± ìƒì„±
    app = build_graph()
    
    # 4. ì „ì²´ ë°ì´í„°ì…‹ ìˆœíšŒ (Question ë‹¨ìœ„ ì‹¤í–‰)
    total_tasks = len(loader)
    logger.info(f"Total tasks to process: {total_tasks}")

    problem_id = 0

    # 4-1. ë¬¸ì œ ê°€ì ¸ì˜¤ê¸° (Flattened Question)
    task_data = loader.get_problem(problem_id)
    
    t_id = task_data['id']
    prompt = task_data['prompt']
    target_dir = task_data['target_dir']
    
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸš€ Processing Task [{problem_id+1}/{total_tasks}] ID: {t_id}")
    logger.info(f"ğŸ“‚ Work Dir: {target_dir}")
    logger.info(f"{'='*60}")
    
    # 4-2. ì´ˆê¸° ìƒíƒœ ì„¤ì • (State Injection)
    inputs = {
        "problem": prompt,          # Stuffed Prompt (Intro + Excel Path + Question)
        "work_dir": target_dir,     # (ì¤‘ìš”) ì—ì´ì „íŠ¸ê°€ ì‘ì—…í•  ì ˆëŒ€ ê²½ë¡œ
        "plan": [],
        "current_step_index": 0,
        "decision": "",
        "context_log": [],
        "variable_inventory": {},
        "tool_retrieved": [],
        "tool_generated": [],
        "feedback_history": [],
        "error": None
    }
    
    # 4-3. ê·¸ë˜í”„ ì‹¤í–‰
    try:
        # recursion_limit: ë³µì¡í•œ ë¬¸ì œì¼ìˆ˜ë¡ ë†’ê²Œ ì¡ì•„ì•¼ í•¨ (50~100)
        for state in app.stream(
            inputs,
            config={
                "recursion_limit": 50,
                "callbacks": [langfuse_handler]},
            stream_mode="values"
        ):
            # logger.info(f"í˜„ì¬ ìƒíƒœ: {state}")
            # print(f"í˜„ì¬ ìƒíƒœ: {state}")

            if state.get("final_answer"):
                print(f"FINAL ANSWER: {state.get("final_answer")}")
            
        # result = app.stream(inputs, config={"recursion_limit": 50})
        
        logger.info(f"âœ… Task {t_id} Completed.")
        
        # (ì„ íƒ) ê²°ê³¼ í™•ì¸ ë¡œì§
        # Analysis: result['context_log'] ë§ˆì§€ë§‰ ë‚´ìš© í™•ì¸
        # Modeling: target_dirì— submission.csv ìƒê²¼ëŠ”ì§€ í™•ì¸
        
    except Exception as e:
        logger.error(f"âŒ Task {t_id} Failed: {e}")
        # ì—ëŸ¬ê°€ ë‚˜ë„ ë‹¤ìŒ ë¬¸ì œë¡œ ê³„ì† ì§„í–‰ (Continue)



def run_test():
    from src.agent.nodes import tool_tester_node
    from src.utils.jupyter_sandbox import JupyterSandbox
    
    print("ğŸ§ª Starting Tool Tester Node Verification...\n")
    
    # í…ŒìŠ¤íŠ¸ìš© ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
    work_dir = "./test_workspace"
    os.makedirs(work_dir, exist_ok=True)

    # ìƒŒë“œë°•ìŠ¤ ì»¨í…ìŠ¤íŠ¸ ì•ˆì—ì„œ ì‹¤í–‰í•´ì•¼ í•¨ (tester_nodeê°€ sandboxë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ)
    with JupyterSandbox(work_dir=work_dir) as sandbox:
        
        # ---------------------------------------------------------
        # CASE 1: ì •ìƒì ì¸ ë„êµ¬ (í…ŒìŠ¤íŠ¸ í†µê³¼ ì˜ˆìƒ)
        # ---------------------------------------------------------
        print("\n" + "="*50)
        print("ğŸŸ¢ [CASE 1] Testing Valid Tool (Should PASS)")
        print("="*50)
        
        valid_tool_state = {
            "work_dir": work_dir,
            "plan": ["Calculate average"],
            "current_step_index": 0,
            "context_log": [],
            # ê°€ì§œë¡œ ìƒì„±ëœ ë„êµ¬ ë¦¬ìŠ¤íŠ¸ ì£¼ì…
            "tool_generated": [
                {
                    "name": "calculate_mean",
                    "docstring": "Calculates the mean of a list.",
                    # ì •ìƒ ì½”ë“œ: ë¦¬ìŠ¤íŠ¸ì˜ í‰ê· ì„ êµ¬í•¨
                    "code": """
def calculate_mean(numbers):
    if not numbers:
        return 0
    return sum(numbers) / len(numbers)
"""
                }
            ]
        }
        
        # ë…¸ë“œ ì‹¤í–‰!
        result_1 = tool_tester_node(valid_tool_state)
        
        print(f"\n[Result]: {result_1['decision']}")
        if result_1['decision'] == "solve":
            print("âœ… CASE 1 PASSED: Correctly accepted valid code.")
        else:
            print(f"âŒ CASE 1 FAILED: Unexpectedly rejected valid code. Error: {result_1.get('error')}")


        # ---------------------------------------------------------
        # CASE 2: ê³ ì¥ë‚œ ë„êµ¬ (í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ì˜ˆìƒ)
        # ---------------------------------------------------------
        print("\n" + "="*50)
        print("ğŸ”´ [CASE 2] Testing Buggy Tool (Should FAIL)")
        print("="*50)
        
        buggy_tool_state = {
            "work_dir": work_dir,
            "plan": ["Calculate average"],
            "current_step_index": 0,
            "context_log": [],
            "tool_generated": [
                {
                    "name": "calculate_mean_buggy",
                    "docstring": "Calculates the mean.",
                    # ë²„ê·¸ ì½”ë“œ: í•©ê³„ë¥¼ êµ¬í•˜ì§€ ì•Šê³  ê·¸ëƒ¥ ê¸¸ì´ë¡œ ë‚˜ëˆ” (ë…¼ë¦¬ ì˜¤ë¥˜)
                    # í˜¹ì€ ë¬¸ë²• ì—ëŸ¬ë¥¼ ë„£ì–´ë´ë„ ë¨
                    "code": """
def calculate_mean_buggy(numbers):
    '''
    Calculates the mean of a list of numbers.
    Input:
        - numbers: list of numbers
    Output:
        - mean of the numbers
    '''
    return len(numbers) 
"""
                }
            ]
        }
        
        # ë…¸ë“œ ì‹¤í–‰!
        result_2 = tool_tester_node(buggy_tool_state)
        
        print(f"\n[Result]: {result_2['decision']}")
        
        if result_2['decision'] == "retry_create":
            print("âœ… CASE 2 PASSED: Correctly caught the bug.")
            print(f"   Error Log Captured: {result_2.get('error')}...") # ì—ëŸ¬ ë‚´ìš© ì¼ë¶€ ì¶œë ¥
        else:
            print("âŒ CASE 2 FAILED: Failed to catch the bug (returned 'solve').")


if __name__ == "__main__":
    main()
    # test_single()
    # run_test()