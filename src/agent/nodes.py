from langchain_openai import ChatOpenAI
import json
from src.config import BASE_URL, API_KEY, MODEL_NAME
from src.agent.state import AgentState
from src.memory.tool_memory import ToolMemory
from src.logger import get_logger
from src.utils.jupyter_sandbox import AgentSandbox
from src.utils.code_parser import parse_tools_from_code
import pprint
from textwrap import dedent
import re
import traceback

logger = get_logger(__name__)
memory = ToolMemory()

# TODO: final answerì—ì„œ ì •ë³´ê°€ ë¶€ì¡±í•˜ë‹¤ê³  íŒë‹¨í•˜ë©´? plan ìˆ˜ì •?
# TODO: tool reusability í–¥ìƒ
# TODO: ì´ë¯¸ retrieveëœ toolì„ ê³ ì¹˜ë ¤ í•˜ëŠ” ê²½ìš°ê°€ ìˆë‚˜? ê³ ì¹˜ëŠ” ê²Œ ë§ë‚˜?

# vLLM ì—°ë™
llm = ChatOpenAI(
    base_url=BASE_URL,
    api_key=API_KEY,
    model=MODEL_NAME,
    temperature=1,
    max_retries=2,
    request_timeout=120,
)


def grounding_node(state: AgentState, sandbox: AgentSandbox):
    """Context Grounding: ì‹¤í–‰ í™˜ê²½ì„ ì½”ë“œë¡œ íƒìƒ‰í•˜ì—¬ Plannerì—ê²Œ ë§¥ë½ì„ ì œê³µí•œë‹¤."""
    problem = state["problem"]
    work_dir = state.get("work_dir", "./")

    logger.info(f"ğŸ” Grounding: Exploring environment in {work_dir}...")

    # Step 1: LLMì—ê²Œ íƒìƒ‰ ì½”ë“œ ìƒì„± ìš”ì²­
    prompt = f"""You are an environment analyst. Before solving a problem, you need to understand the available resources.

Given a problem description and a working directory, write Python code to explore and summarize the environment.

## Problem:
{problem}

## Working Directory: {work_dir}

## Instructions:
1. List all files and subdirectories in the working directory.
2. For each data file found (CSV, Excel, JSON, Parquet, etc.):
   - Load it and print its shape, column names, dtypes, and first 3 rows.
3. For each text file (TXT, MD, etc.):
   - Print its contents (first 500 chars if too long).
4. Print a summary of what resources are available.
5. Use try/except to handle any errors gracefully.
6. Keep output concise but informative.

```python
# Your exploration code here
```
"""

    response = llm.invoke(prompt).content

    # ì½”ë“œ ì¶”ì¶œ
    code_match = re.search(r'```python(.*?)```', response, re.DOTALL)
    if code_match:
        code = code_match.group(1).strip()
    else:
        # Fallback: ê¸°ë³¸ íƒìƒ‰ ì½”ë“œ
        code = f"""
import os
print("=== Files in working directory ===")
for f in sorted(os.listdir('{work_dir}')):
    fpath = os.path.join('{work_dir}', f)
    size = os.path.getsize(fpath) if os.path.isfile(fpath) else 'DIR'
    print(f"  {{f}} ({{size}})")
"""

    # Step 2: ì½”ë“œ ì‹¤í–‰
    result = sandbox.run_code(code, mode="permanent")

    grounding_context = ""
    if result["stdout"]:
        grounding_context = result["stdout"][:3000]  # í† í° ì ˆì•½
        logger.info(f"   âœ… Grounding complete ({len(grounding_context)} chars)")
    else:
        grounding_context = "No output from environment exploration."
        logger.warning(f"   âš ï¸ Grounding produced no output.")

    if result["stderr"]:
        logger.warning(f"   âš ï¸ Grounding errors: {result['stderr'][:200]}")
        grounding_context += f"\n[Errors]: {result['stderr'][:500]}"

    return {"grounding_context": grounding_context}


def planner_node(state: AgentState):
    logger.info(f"Planning task...")


# data analysis taskì—ì„œëŠ” íŒŒì¼ì„ ì§ì ‘ ì½ì–´ì„œ ë„£ì–´ì£¼ê³ ,
# data modeling taskì—ì„œëŠ” í´ë” ê²½ë¡œë§Œ ì£¼ê³  íŒŒì¼ì„ LLMì´ ì§ì ‘ ì½ì–´ì•¼ í•¨
# 3. **File I/O**:
    # - The first step of the plan has to be checking the file list in the working directory.
    # - You generally do NOT know the exact filenames yet. You must check them first.
    # - The second step of the plan has to be loading the relevant data file(s) identified in the first step.
    
    prompt = f"""Analyze the User Request and break it down into step-by-step Python coding tasks.
                Also, the subtasks should be general so that it can be applicable to other similar problems.
                You do **not** need to define specific Python function signatures at this stage.
                Instead, clearly define each subtask with sufficient abstraction and generality, enabling later selection or generation of reusable tools (Python functions).

                Follow these guidelines strictly:

                1. **Clearly define each subtask**:
                    - Each subtask must represent an independent, fundamental step.
                    - **Step Type**: Assign a `type` to each step: `"tool"` or `"reasoning"`.
                        - `"tool"`: Use for heavy calculations, simulations, file I/O, dataframe manipulation, or complex algorithmic tasks (e.g., shortest path).
                        - `"reasoning"`: Use for logical deduction, simplifying fractions, summarizing results, establishing equations, or simple arithmetic that doesn't need Python.
                    - Clearly describe the input/output data required and intermediate values.

                2. **Abstract and general**:
                    - Design subtasks broadly enough to encourage reuse across multiple similar queries.
                    - Avoid overly narrow or query-specific subtasks.

                ---

                ## Query:
                ===query===

                ---
                Respond **strictly** in the following **JSON format**:

                ```json
                [
                    {{
                        "name": "Name of subtask 1",
                        "type": "tool" or "reasoning",
                        "description": "A brief, abstract description of subtask 1.",
                        "input": {{
                            "input_var_1": {{
                                "description": "Explain what this variable represents.",
                                "type": "data type (e.g., float, list[float])",
                                "shape": "scalar / list / expression / etc.",
                                "example": "example value (optional)
                            }}
                        }},
                        "output": {{
                            "output_var_1": {{
                                "description": "What this output represents.",
                                "type": "data type",
                                "shape": "scalar / list / expression / etc.",
                                "example": "example output (optional)"
                            }}
                        }}
                    }},
                    # add more subtasks as needed
                ]
                ```
            """


    prompt = f"""Analyze the User Request and break it down into subtasks.

Follow these guidelines strictly:

1. **Clearly define each subtask**:
    - Each subtask must represent an independent, fundamental step.
    - Clearly describe the input/output data required and intermediate values.
    - **Step Type**: Assign a `type` to each step: `"tool"` or `"reasoning"`.
        - `"tool"`: Use ONLY for heavy calculations, simulations, file I/O, dataframe manipulation, or complex algorithmic tasks that genuinely benefit from Python code execution (e.g., Monte Carlo simulation, large combinatorics, graph algorithms).
        - `"reasoning"`: Use for logical deduction, simplifying fractions, establishing equations, simple arithmetic, counting small cases, summarizing results, or any step where a human would solve it by thinking rather than coding.
    - **CRITICAL**: Default to `"reasoning"` unless the step clearly requires code. Most math problem steps are reasoning.
    - **CRITICAL**: If the problem description includes `[asy]` code, you MUST create a specific subtask to parse or analyze this code to extract geometric parameters (e.g., grid size, coordinates, labels). Do not ignore it.
    - **CRITICAL**: If the answer is expected to be a fraction, ensure there is a step to simplify it (as `"reasoning"` type).

---

## Environment Context:
===grounding===

## Query:
===query===

---
Respond **strictly** in the following **JSON format**:

```json
[
    {{
        "name": "Name of subtask 1",
        "type": "tool" or "reasoning",
        "description": "A description of subtask 1."
    }},
    # add more subtasks as needed
]
```
"""
    # prompt = dedent(prompt)
    while True:
        try:
            grounding = state.get('grounding_context', 'No environment context available.')
            filled = prompt.replace('===query===', state['problem']).replace('===grounding===', grounding)
            plan = llm.invoke(filled).content

            if '```json' in plan:
                plan = plan.split('```json')[1].split('```')[0]

            plan = json.loads(plan)
            break
        except:
            # try:
            #     plan = eval(plan)
            #     break
            # except:
            #     logger.error(f"{'='*20} [Plan Parse Error] {'='*20}\n")
            logger.error(f"{'='*20} [Plan Parse Error] {'='*20}\n")

    # print(f"\n{'='*20} [LLM RAW OUTPUT] {'='*20}")
    # pprint.pp(plan)
    # print(f"{'='*60}\n")
    
    return {"plan": plan, "current_step_index": 0, "context_log": []}


def tool_manager_node(state: AgentState):
    plan = state['plan']
    idx = state['current_step_index']
    current_task = plan[idx]
    logger.info(f"ğŸ” Checking tools for {idx+1}/{len(plan)} tasks...")

    # 0. Reasoning Stepì¸ ê²½ìš° ë°”ë¡œ bypass
    if current_task.get('type') == 'reasoning':
        logger.info("ğŸ§  Reasoning Task detected. Skipping tool retrieval.")
        return {
            "tool_retrieved": [],
            "tool_generated": [],
            "decision": "reason"
        }

    # 1. ë²¡í„° DBì—ì„œ ê²€ìƒ‰
    candidates = memory.search_tools(current_task['description'], k=5)

    if not candidates:
        logger.info("âŒ No candidates found in DB.")
        return {
            "tool_retrieved": [],
            "decision": "create"
        }

    candidates_info = "\n".join([
        f"[{i}] Name: {c['name']}\n    Description: {c['docstring']}\n    Code: {c['code']}"
        for i, c in enumerate(candidates)
    ])

    prompt = (
        f"You are a Tool Manager. Your goal is to decide whether tools are necessary or not, and if necessary, to reuse an existing tool or create a new one.\n\n"
        f"ğŸ¯ **Current Task**: {current_task['description']}\n"
        f"ğŸ” **Candidate Tools found in Memory**:\n"
        f"{candidates_info}\n\n"
        f"ğŸ›‘ **Instruction**:\n"
        f"1. Analyze if any of the candidates PERFECTLY matches the Current Task.\n"
        f"2. Consider if the input variables required by the tool are available.\n"
        f"3. If a match is found, return the index number (e.g., '0', '1').\n"
        f"4. If NONE match or strictly require modification return 'CREATE'.\n\n"
        f"5. If tool is not necessary, return 'NO TOOL'.\n\n"
        f"Answer ONLY with the index number, 'CREATE' or 'NO TOOL'."
    )

    response = llm.invoke(prompt).content.strip()

    if response.upper() == "CREATE":
        logger.info("ğŸ¤” Candidates rejected. Creating new tool.")
        return {"decision": "create"}
    elif response.upper() == "NO TOOL":
        logger.info("ğŸ¤” Candidates rejected. No tool is needed.")
        return {
            "tool_retrieved": [],
            "tool_generated": [],
            "decision": "solve"
        }
    else:
        try:
            # ì¸ë±ìŠ¤ë¡œ ì„ íƒëœ ë„êµ¬ ê°€ì ¸ì˜¤ê¸°
            selected_idx = int(response)
            selected_tool = candidates[selected_idx]
            
            logger.info(f"â™»ï¸ Reusing tool: {selected_tool['name']}")
            return {
                "tool_retrieved": [selected_tool],
                "tool_generated": [],
                "decision": "solve"
            }
        except ValueError:
            # LLMì´ ì´ìƒí•œ ë‹µì„ í•˜ë©´ ì•ˆì „í•˜ê²Œ ìƒì„±ìœ¼ë¡œ ì´ë™
            return {
                "tool_retrieved": [],
                "tool_generated": [],
                "decision": "create"
            }
    

def tool_creator_node(state: AgentState):
    plan = state['plan']
    idx = state['current_step_index']
    current_task = plan[idx]
    history = state.get("feedback_history", [])
    context_log = state.get("context_log", [])

    if history:
        # ğŸ”„ ìˆ˜ì • ëª¨ë“œ (Fix Mode with History)
        logger.info(f"ğŸ”§ Fixing tool based on {len(history)} past failures...")
        
        # íˆìŠ¤í† ë¦¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì˜ˆì˜ê²Œ í¬ë§·íŒ…
        history_summary = ""
        for item in history:
            history_summary += f"=== âŒ ATTEMPT (Source: {item['source'].upper()}) ===\n"
            history_summary += f"[Tool Code Used]:\n{item.get('tool_code', 'N/A')}\n\n"
            history_summary += f"[Test/Exec Code]:\n{item.get('test_code') or item.get('execution_code')}\n\n"
            history_summary += f"[Error Log]:\n{item['error_log']}\n"
            history_summary += f"==========================================================\n"
    
        prompt = f"""You are a Senior Python Data Engineer & Debugging Expert.
            Your goal is to fix a broken tool based on the provided error log.

            ---
            ### 1. CONTEXT
            **Original Task:**
            {current_task}
            
            **Reasoning Context:**
            {json.dumps(context_log, indent=2, default=str)}

            **Previous Attempts & Failures:**
            {history_summary}

            ---
            ### 2. ğŸ•µï¸â€â™‚ï¸ DIAGNOSTIC CHECKLIST (Review these BEFORE fixing)

            Run through this mental checklist to identify the root cause. Do NOT assume data quality is perfect.

            **A. Type Mismatch (Most Common)**
            - âŒ **Issue:** Treating `int`/`str` as `datetime` (e.g., `AttributeError: 'int' object has no attribute 'year'`).
            - âœ… **Fix:** Explicitly convert columns using `pd.to_datetime()` or `astype()` before processing.
            - âŒ **Issue:** Treating a scalar (float) as a list/iterable.
            - âœ… **Fix:** Wrap scalar values in a list `[value]` if iteration is needed.

            **B. Data Structure & Keys**
            - âŒ **Issue:** `KeyError` or `IndexError` (Assuming a column name or index exists).
            - âœ… **Fix:** Check column names (case-sensitivity, strip whitespace). Use `.get()` or check `if col in df.columns`.
            - âŒ **Issue:** Applying DataFrame methods to a Series (or vice versa).

            **C. Scope & Definitions**
            - âŒ **Issue:** `NameError` (Using variables/imports not defined inside the function).
            - âœ… **Fix:** Ensure all imports (e.g., `import pandas as pd`) and variables are defined INSIDE the function or passed as arguments.

            **D. Logic & Math**
            - âŒ **Issue:** Division by zero, or `NaN` propagation.
            - âœ… **Fix:** Handle edge cases (empty data, `NaN` values) using `.fillna()` or `if not data.empty`.

            ---
            ### 3. ğŸ“ YOUR TASK (Chain of Thought)

            **Step 1: ANALYSIS**
            - Identify the specific line number from the Error Log.
            - Explain WHY the error occurred based on the "Diagnostic Checklist" above.
            - Explicitly state what assumption in the old code was wrong (e.g., "Code assumed 'date' column was datetime object, but it was likely an integer/string").

            **Step 2: REFACTORING**
            - Write the corrected Python code.
            - **CRITICAL:** Add defensive coding (e.g., explicit type conversion, check for empty df) to prevent this from happening again.
            - Include the necessary imports within the code.

            ---
            ### 4. OUTPUT FORMAT

            ğŸ”´ **OUTPUT FORMAT**:
                <analysis>your diagnosis here</analysis>
                <main_func>name of the function to call</main_func>
                <description>what this tool does</description>
                ```python
                # your code
                # DO NOT INCLUDE TEST CODE
                # ONLY INCLUDE ORIGINAL TOOL CODE
                ```
        """

        prompt = dedent(prompt)

    else:
        # âœ¨ ìƒì„± ëª¨ë“œ (Create Mode)
        logger.info("ğŸš€ Creating new tool...")
    
        prompt = (
            f"You are a generic Python function generator.\n"
            f"Task to solve:\n"
            f"{json.dumps(current_task, indent=2)}\n\n"
            f"Context from previous reasoning steps:\n"
            f"{json.dumps(context_log, indent=2, default=str)}\n\n"
            f"Requirements:\n"
            f"1. Create a Python function for the task.\n"
            f"2. The function must be independent and self-contained.\n"
            f"3. Return the result as a Python code snippet with tools (function definitions) in it.\n"
            f"4. Include the docstring for the function.\n"
            f"ğŸ”´ **OUTPUT FORMAT (Strict JSON)**:\n"
            f"<analysis>your analysis here</analysis>\n"
            f"<main_func>name of the function to call</main_func>\n"
            f"<description>what this tool does</description>\n"
            f"```python\n"
            f"# your code\n"
            f"```"
        )
    
    max_attempt = 3
    for attempt in range(max_attempt):
        try:
            response = llm.invoke(prompt).content

            analysis_match = re.search(r"<analysis>(.*?)</analysis>", response, re.DOTALL)
            desc_match = re.search(r"<description>(.*?)</description>", response, re.DOTALL)
            func_match = re.search(r"<main_func>(.*?)</main_func>", response, re.DOTALL)
            code_match = re.search(r"```python(.*?)```", response, re.DOTALL)
            
            if not desc_match or not func_match or not code_match:
                raise ValueError("Failed to extract tool information")
            
            description = desc_match.group(1).strip()
            name = func_match.group(1).strip()
            code = code_match.group(1).strip()

        except Exception as e:
            if attempt == max_attempt - 1:
                logger.error(f"Tool generation failed: {e}")
                return {
                    # decision?
                    "tool_generated": [],
                    "error": "Parsing Failed"
                }
            else:
                continue

    logger.info(f"âœ… Generated a new tool.")
    
    return {
        "tool_generated": [{
            "name": name,
            "code": code,
            "docstring": description
        }],
        "tool_retrieved": [],
        "error": None
    }



def tool_tester_node(state: AgentState, sandbox: AgentSandbox):
    tools = state['tool_generated']
    work_dir = state['work_dir']
    history = state.get("feedback_history", [])
    
    logger.info(f"ğŸ§ª Starting Sequential Testing for {len(tools)} tools...")
    
    # ---------------------------------------------------------
    # 1ë‹¨ê³„: ëª¨ë“  í•¨ìˆ˜ ì •ì˜(Definition) ë¡œë“œ
    # (ì„œë¡œ ì˜ì¡´ì„±ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¼ë‹¨ ë‹¤ ë©”ëª¨ë¦¬ì— ì˜¬ë¦½ë‹ˆë‹¤)
    # ---------------------------------------------------------
    all_defs = "\n\n".join([t['code'] for t in tools])
    
    # ì •ì˜ ë‹¨ê³„ì—ì„œ ì—ëŸ¬ë‚˜ë©´ ë°”ë¡œ Creatorë¡œ ë°˜ë ¤ (Syntax Error ë“±)
    def_result = sandbox.run_code(all_defs, mode="temporary")
    sandbox.cleanup_test_kernel()
    if def_result['stderr']:
        logger.error(f"âŒ Syntax Error in Definitions: {def_result['stderr']}")
        return {
            "error": f"Syntax Error during function definition:\n{def_result['stderr']}",
            "decision": "retry_create"
        }

    # ---------------------------------------------------------
    # 2ë‹¨ê³„: í•˜ë‚˜ì”© ìˆœíšŒí•˜ë©° ìœ ë‹› í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    # ---------------------------------------------------------
    failed_reports = []
    max_attempt = 3

    if history:
        history_summary = ""
        for item in history:
            history_summary += f"=== âŒ ATTEMPT (Source: {item['source'].upper()}) ===\n"
            history_summary += f"[Tool Code Used]:\n{item.get('tool_code', 'N/A')}\n\n"
            history_summary += f"[Test/Exec Code]:\n{item.get('test_code') or item.get('execution_code')}\n\n"
            history_summary += f"[Error Log]:\n{item['error_log']}\n"
            history_summary += f"==========================================================\n"
    
    for tool in tools:
        logger.info(f"   ğŸ‘‰ Testing individual tool: {tool['name']}")
        
        # (A) í…ŒìŠ¤íŠ¸ ì½”ë“œ ìƒì„± (ì´ ë„êµ¬ í•˜ë‚˜ì—ë§Œ ì§‘ì¤‘)
        prompt = f"""
            Write a Python Unit Test for the function: `{tool['name']}`.
            The unit tests should test whether the function is logically correct as intended, not only the syntax.
            Function Code:
            {tool['code']}


            REQUIREMENTS:
            1. Create minimal dummy data to verify logic.
            2. Use `assert` statements.
            3. Include necessary imports.
            4. Include the tool code as is. Just call the function and assert the result.
            5. You MUST run the test with: `unittest.main(argv=[''], exit=False)`
        """

        if history:
            prompt += f"\n\nPrevious Attempts & Failures:\n{history_summary}"

        prompt += """
        
            OUTPUT FORMAT:
            <thought>rationale on the test case</thought>
            ```python
            # your code
            ```
            """
        
        prompt = dedent(prompt)
        test_code = llm.invoke(prompt).content

        if '```python' in test_code:
            test_code = test_code.split('```python')[1].split('```')[0]
        elif '```' in test_code: # fallback
            test_code = test_code.split('```')[1].split('```')[0]

        # print(f"\n{'='*20} [LLM RAW OUTPUT] {'='*20}")
        # print(test_code)
        # print(f"{'='*60}\n")
        
        # (B) ì‹¤í–‰ (ì´ë¯¸ í•¨ìˆ˜ë“¤ì€ 1ë‹¨ê³„ì—ì„œ ë¡œë“œë˜ì—ˆìœ¼ë¯€ë¡œ í…ŒìŠ¤íŠ¸ ì½”ë“œë§Œ ì‹¤í–‰)
        try:
            result = sandbox.run_code(test_code, mode="temporary")
            sandbox.cleanup_test_kernel()
        except Exception as e:
            error_msg = traceback.format_exc()
            logger.error(f"FATAL ERROR: {error_msg}")
            return {
                "decision": "retry_create",
                "error": error_msg
            }
        
        # (C) ê²°ê³¼ ê¸°ë¡
        if result['stderr']:
            if "OK" in result['stderr'] and "FAILED" not in result['stderr']:
                logger.info(f"      âœ… Passed: {tool['name']}")
            else:
                history = state.get("feedback_history", [])
                current_feedback = {
                    "source": "tester",
                    "tool_code": tool['code'],
                    "test_code": test_code,
                    "error_log": result['stderr']
                }

                new_history = history + [current_feedback]

                if len(new_history) > 3:
                    new_history = new_history[-3:]

                logger.warning(f"      âŒ Failed: {tool['name']}")

                logger.debug(
                    f"--- Tool: {tool['name']} ---\n"
                    f"Error: {result['stderr']}\n"
                    f"Test Code Used:\n{test_code}\n"
                )
                failed_reports.append(
                    f"--- Tool: {tool['name']} ---\n"
                    f"Error: {result['stderr']}\n"
                    f"Test Code Used:\n{test_code}\n"
                )

                return {
                    "decision": "retry_create",
                    "feedback_history": new_history,
                    "error": result['stderr']
                }
        else:
            logger.info(f"      âœ… Passed: {tool['name']}")

    # ---------------------------------------------------------
    # 3ë‹¨ê³„: ê²°ê³¼ ì·¨í•© ë° ê²°ì •
    # ---------------------------------------------------------
    if failed_reports:
        # í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨í•˜ë©´ ë°˜ë ¤
        error_summary = "\n".join(failed_reports)
        return {
            "error": f"Unit Tests Failed for the following tools:\n{error_summary}",
            "decision": "retry_create",
            # ì‹¤íŒ¨ ë¡œê·¸ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
            "context_log": state['context_log'] + [f"Test Failures:\n{error_summary}"]
        }
    else:
        # ëª¨ë‘ í†µê³¼!
        return {
            "error": None,
            "decision": "solve"
        }


def solver_node(state: AgentState, sandbox: AgentSandbox):
    logger.info("Running solver...")
    """
    [Solver]
    ì‹¤í–‰ ê²°ê³¼ë¥¼ ë³´ê³  ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
    """
    plan = state['plan']
    current_idx = state['current_step_index']
    current_task = plan[current_idx]
    inventory = state.get("variable_inventory", {})
    final_context = sandbox.get_final_context()
    tools = state.get("tool_generated", []) + state.get("tool_retrieved", [])
    context_log = state.get("context_log", [])
    
    # 1. ì •ì˜ ë¡œë“œ (ì´ë¯¸ Testerë‚˜ ì´ì „ ë‹¨ê³„ì—ì„œ í–ˆê² ì§€ë§Œ ì•ˆì „í•˜ê²Œ ë‹¤ì‹œ)
    all_defs = "\n\n".join([t['code'] for t in tools])
    tool_desc = "\n".join([f"- {t['name']}: {t['docstring']}" for t in tools])

    max_retries = 3
    temp_history = []

    for attempt in range(max_retries):    
        # 2. ì‹¤ì „ ì‹¤í–‰ ì½”ë“œ ìƒì„±
        prompt = (
            f"Task: {current_task}\nTools:\n{tool_desc}\nVariables currently in memory: {inventory}\n"
            f"Reasoning Context: {json.dumps(context_log, indent=2, default=str)}\n"
            f"Write code to solve the task using real data variables.\n"
            f"Save result to new variable."
            f"Include necessary imports."
        )

        # solver ë‚´ë¶€ loop; ì´ì „ ì‹¤íŒ¨ ë¡œê·¸ ì¶”ê°€
        if temp_history:
            prompt += "\n\nğŸš« PREVIOUS FAILED ATTEMPTS (LEARN FROM MISTAKES):\n"
            for h in temp_history:
                prompt += f"- Code:\n{h['exec_code']}\n"
                prompt += f"- Error:\n{h['error']}\n\n"
            prompt += "ğŸš¨ ERROR ANALYSIS: The tool code is fixed. Focus on fixing YOUR calling arguments or logic."

        exec_code = llm.invoke(prompt).content

        if '```python' in exec_code:
            exec_code = exec_code.split('```python')[1].split('```')[0]
        elif '```' in exec_code: # fallback
            exec_code = exec_code.split('```')[1].split('```')[0]
        
        # 3. ì‹¤í–‰
        full_code = f"{all_defs}\n\n# Execution\n{exec_code}"
        res = sandbox.run_code(full_code, mode="permanent")
    
        if res['stderr']:
            logger.error(f"âŒ Solver Execution Failed: {res['stderr']}")
            logger.error(f"   Code used:\n{exec_code}")

            temp_history.append({
                "exec_code": exec_code,
                "error": res['stderr']
            })

            if attempt == max_retries - 1:
                logger.error("Solver max retries exceeded")

                history = state.get("feedback_history", [])
                current_feedback = {
                    "source": "solver",
                    "tool_code": all_defs,
                    "execution_code": exec_code,
                    "error_log": res['stderr']
                }

                new_history = history + [current_feedback]

                if len(new_history) > 3:
                    new_history = new_history[-3:]
                
                return {
                    "decision": "retry_create", 
                    "feedback_history": new_history,
                    "error": f"Runtime Error: {res['stderr']}"
                }
            
            continue
        
        # 4. ë³€ìˆ˜ ì—…ë°ì´íŠ¸ (Inspection)
        logger.info("âœ…Solver Execution Succeeded")
        inspect_code = "import json; print(json.dumps({k:type(v).__name__ for k,v in globals().items() if not k.startswith('_')}))"
        try:
            insp_res = sandbox.run_code(inspect_code, mode="permanent")
            new_inv = json.loads(insp_res['stdout'])
        except:
            new_inv = inventory

        # 5. ì„±ê³µ ì‹œ ë„êµ¬ ì €ì¥ (ìƒì„±ëœ ê²½ìš°ë§Œ)
        if state.get("tool_generated"):
            for t in state.get("tool_generated"):
                try:
                    memory.add_tool({
                        "name":t['name'],
                        "code":t['code'],
                        "docstring":t['docstring']
                    })
                except Exception as e:
                    logger.error(f"Failed to add tool to memory: {e}")

        # 6. ë‹¤ìŒ ìŠ¤í… íŒë³„
        next_idx = state['current_step_index'] + 1
        
        return {
            "decision": "continue", 
            "current_step_index": next_idx, 
            "variable_inventory": new_inv,
            "tool_generated": [],
            "tool_retrieved": [],
            "feedback_history": [],
            "error": None
        }


def reasoner_node(state: AgentState):
    plan = state['plan']
    idx = state['current_step_index']
    current_task = plan[idx]
    context_log = state.get("context_log", [])
    inventory = state.get("variable_inventory", {})
    
    logger.info(f"ğŸ§  Reasoning about task: {current_task['description']}")

    prompt = f"""You are a Logic & Reasoning Engine.
    Your goal is to solve the current subtask using logical deduction, arithmetic, or summarization, WITHOUT writing Python code.

    ---
    ### Current Task:
    {current_task['description']}

    ### Context Variables (from previous steps):
    {json.dumps(inventory, indent=2, default=str)}

    ### Previous Reasoning/Context:
    {json.dumps(context_log, indent=2, default=str)}

    ---
    Based on the above, provide the result or conclusion for the current task.
    Be concise and specific.
    If you calculate a value, state it clearly.
    """

    response = llm.invoke(prompt).content
    logger.info(f"ğŸ’¡ Reasoning Result: {response}")

    # Log update
    new_log = context_log + [f"Step {idx+1} [Reasoning]: {response}"]

    return {
        "decision": "continue",
        "current_step_index": idx + 1,
        "context_log": new_log,
        "tool_generated": [],
        "tool_retrieved": [],
        "error": None
    }


def final_answer_node(state: AgentState, sandbox: AgentSandbox):
    query = state['problem']
    inventory = state['variable_inventory'] # Solverë“¤ì´ ì—´ì‹¬íˆ ëª¨ì€ ê²°ê³¼ê°’ë“¤
    context_log = state.get("context_log", [])
    final_context = sandbox.get_final_context()
    
    logger.info("ğŸ Generating Final Answer...")

    # LLMì—ê²Œ "ìë£Œ ì¤„ê²Œ, ë‹µ ì¨ì¤˜"ë¼ê³  ìš”ì²­
    prompt = (
        f"You are a helpful Data Analyst Assistant.\n"
        f"Original Question: {query}\n\n"
        f"We have processed the data and executed the plan. "
        f"Here are the collected variables and their values:\n"
        f"{json.dumps(final_context, indent=2, default=str)}\n\n"
        f"Here is the reasoning log from non-coding steps:\n"
        f"{json.dumps(context_log, indent=2, default=str)}\n\n"
        f"MISSION:\n"
        f"1. Synthesize the information from the variables AND reasoning log to answer the Original Question.\n"
        f"2. Be direct and concise.\n"
        f"3. If the answer is a specific number or list found in variables, provide it clearly.\n"
        f"4. If the answer is a fraction, you MUST simplify it to lowest terms (e.g., 91/21 -> 13/3).\n"
        f"5. Do NOT show python code or internal variable names in the final answer."
    )
    
    prompt = f"""You are a math expert. Your task is to answer the following question with your reasoning.
                The answer has to be in one of following formats: integer, float, complex number, (numeric) string, (LaTeX expression) string.
                You must put your answer in $\\boxed{{}}$

                Think step by step.
                ---
                ## Problem:
                {query}

                ## Here are some variables collected from intermediate steps. You can use these variables for your reasoning.
                {json.dumps(final_context, indent=2, default=str)}

                ## Here is the reasoning log from non-coding steps:
                {json.dumps(context_log, indent=2, default=str)}

                ## Reasoning:
                <Your step-by-step explanation>

                ## Answer:
                <Your answer>
            """
    
    prompt = dedent(prompt)
    
    response = llm.invoke(prompt).content

    print(response)
    
    return {"final_answer": response}